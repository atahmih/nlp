{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc169c45",
   "metadata": {},
   "source": [
    "# Training GPT-1 on Tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1516f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27c1d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74dca121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# get unique chars\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef4fb8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526874a8",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6645c3",
   "metadata": {},
   "source": [
    "In our implementation, we are simply converting individual characters into integers, since we are building a character-level model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe7ec398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # convert a string into a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # convert a list of integers into a string\n",
    "print(encode('hi there'))\n",
    "print(decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff23046b",
   "metadata": {},
   "source": [
    "Tokenizing the entire set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4f07350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737ba31",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10ddcf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4bcc78",
   "metadata": {},
   "source": [
    "# 'Chunking' the data <br>\n",
    "Feeding the entire data into the model at once will be computationally infeasible. Hence we sample random chunks and feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a852a521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3dc64",
   "metadata": {},
   "source": [
    "A chunk, in essence, is multiple training examples because the model is trained to read each incremental sequence of characters and predict the next in sequence <br>\n",
    "e.g. in 18, the prediction is 47. in 18,47 the prediction is 56, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd5886aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), the target is 47\n",
      "when input is tensor([18, 47]), the target is 56\n",
      "when input is tensor([18, 47, 56]), the target is 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # the first block size chars\n",
    "y = train_data[1:block_size+1] # the next block size char offset by 1\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # chars up to and including t\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, the target is {target}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef517113",
   "metadata": {},
   "source": [
    "Training in this way serves two purposes <br>\n",
    "1. Computational efficiency <br>\n",
    "2. Familiarize the transformer with context from as little as 1, all the way to its blocksize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68100fd",
   "metadata": {},
   "source": [
    "Batching our data for parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d59f356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "------\n",
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate batch size numbers of random offsets\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # the first block size chars, stacked from 1D tensors into rows\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # the offset \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context.tolist()} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0f1b7",
   "metadata": {},
   "source": [
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871ea74",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a911a27",
   "metadata": {},
   "source": [
    "Bi-Gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf567e",
   "metadata": {},
   "source": [
    "In the code, the constructor creates an embedding table. <br> \n",
    "When we pass idx in the forward pass, every int in the input refers to the embedding table and gets a row corresponding to the index. <br>\n",
    "This is returns in a batch, time, channel (B, T, C) tensor ie (4,8,65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3a8215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) \n",
    "        if targets  is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # batch, time, channels\n",
    "            logits = logits.view(B*T, C) # stretching the array into 2D\n",
    "            targets = targets.view(B*T) # stretching to 1D\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            # get the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # we are feeding a batch of zeros (newline chars)\n",
    "\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) # [0] get the content without the batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1043bd",
   "metadata": {},
   "source": [
    "Training the Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37e697b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "063bc8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecor\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6631ed2",
   "metadata": {},
   "source": [
    "# Standard Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae49a549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.4801, val loss 4.4801\n",
      "Step 300: train loss 2.5404, val loss 2.5566\n",
      "Step 600: train loss 2.5160, val loss 2.5335\n",
      "Step 900: train loss 2.4967, val loss 2.5149\n",
      "Step 1200: train loss 2.5106, val loss 2.5254\n",
      "Step 1500: train loss 2.4853, val loss 2.5109\n",
      "Step 1800: train loss 2.4966, val loss 2.5198\n",
      "Step 2100: train loss 2.4949, val loss 2.5100\n",
      "Step 2400: train loss 2.4937, val loss 2.5102\n",
      "Step 2700: train loss 2.5040, val loss 2.5114\n",
      "\n",
      "Foasth prsexpizequppathel\n",
      "GOMUKEE&CKIOMINCHUKEENORineg aggellprdrrvetecowhrthy;\n",
      "The?\n",
      "TyONGrsothy,\n",
      "D HPayomind ppry Pad avend\n",
      "Wh T:\n",
      "TRDUMEYOf bykncaknd-htcthy\n",
      "BORYOFRIOMBOf gwisexprenlbuststlant'GSCKENGBETHURK:\n",
      "MONTRKIIDUKIEUSLUKI:\n",
      "Why!\n",
      "TICllppp'ly BEThapy HEED:\n",
      "PORKINGLnk&CHBRI's,\n",
      "B&CllDWlerd,\n",
      "uqughold crrayf\n",
      "\n",
      "PESThe\n",
      "TJUSENCTINIXEnd!\n",
      "3 bald-YOLYowmy!\n",
      "HENRKVOfakthindy;\n",
      "MBERK:\n",
      "QULANGEGHLOfourknghm H:\n",
      "YTIppmplkedextherex'digavitfuthrd.'GENUS:\n",
      "My,\n",
      "WhindKINGABedigerdjurd.\n",
      "MERMjulKENGickequr.F INIS:\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c107b1",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a2423",
   "metadata": {},
   "source": [
    "A Mathematical Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5d61529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 \n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3307fde",
   "metadata": {},
   "source": [
    "We have 8 tokens in a batch that initially don't 'talk' to each other <br>\n",
    "We'd like to couple them in a way that the token in e.g. 5th location should not communicate with the token in the 6th, 7th, and 8th location<br>because those are in future locations and we are trying to predict the future token<br>\n",
    "The token in 5th position should only talk to those in the 4th, 3rd, 2nd, and 1st position i.e. info only flows from previous timesteps <br>\n",
    "to the current timestep <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1eb74",
   "metadata": {},
   "source": [
    "The easiest way to communicate is to do an average of all previous elements <br>\n",
    "e.g. in the 5th position, we want to take the average of channels in the 4th, 3rd, 2nd, and 1st positions <br>\n",
    "This becomes sort of a feature vector that summarizes the 5th in the context of its history <br>\n",
    "This 'average' is, of course, a very simplified reduction of information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe2e03",
   "metadata": {},
   "source": [
    "Now, for every t-th token in a sequence, we want to calculate the average of all vectors in previous tokens and at the current token <br>\n",
    "i.e. we want x[b,t] = mean_{i<=t} x[b, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cf82fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # x bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # everything up to and including the t-th token\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8101a77",
   "metadata": {},
   "source": [
    "Making it efficient with MatMul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf534dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'{a=}')\n",
    "print(f'{b=}')\n",
    "print(f'{c=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca70a3",
   "metadata": {},
   "source": [
    "torch has a function for lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd1a4e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'{a=}')\n",
    "print(f'{b=}')\n",
    "print(f'{c=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e983ad",
   "metadata": {},
   "source": [
    "By design of matmul, the first row of a dot prod first col of b <br>\n",
    "But since row 0 of a = 1, 0, 0, the 0, 0 essentially means that 6, 6, of col 1 in b are ignored and we get 2 + 0 + 0<br>\n",
    "Now, with row 1 of a = 1, 1, 0, the last 0 means the last 6 of col 1 gets ignore and we get 2 + 6 = 8 <br>\n",
    "And, with row 2 of a = 1, 1, 1, we sum up everything and get 2 + 6 + 6 = 14. <br>\n",
    "Same logic applies for col 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff462ef",
   "metadata": {},
   "source": [
    "Depending on the number of zeros and ones, we are therefore doing a sum of the variables in b and depositing in c <br>\n",
    "We can similarly do average of the rows of b in an incremental fashion by normalizing the rows of a so they sum to 1. <br>\n",
    "Then we get an average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bfa9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'{a=}')\n",
    "print(f'{b=}')\n",
    "print(f'{c=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9283a51",
   "metadata": {},
   "source": [
    "We are now getting the average of the rows in the next row (excluding row 0) i.e.<br>\n",
    "4.0 = (2+6) / 2 and 5.5 = (7+4) / 2 <br>\n",
    "4.6 = (2+6+6) / 3 and 5.3 = (7+4+5) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4ce2f",
   "metadata": {},
   "source": [
    "Vectorizing our bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6084f441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T)) # weights\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "697c964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x # (T, T) @ (B, T, C) ---> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d96f22",
   "metadata": {},
   "source": [
    "Another way of writing it: Using Softmax to normalize. <br>\n",
    "In max_fill, we say for all elements where tril=0, make them -inf <br>\n",
    "Softmax then normalizes the rows to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c19994",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m wei = F.softmax(wei, dim=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m xbow3 = wei @ x\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxbow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxbow3\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39732928",
   "metadata": {},
   "source": [
    "In self attention terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f8c45",
   "metadata": {},
   "source": [
    "wei = torch.zeros() tells us that initially, the current tokens have zero affinity for past tokens. These token affinities will become data dependent and update each other during training <br>\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) we say we won't aggregate anything from future tokens, i.e. the future can't communicate with the past <br>\n",
    "wei = F.softmax(wei, dim=1) we then aggregate their values depending on how interesting they find each other <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4686f0",
   "metadata": {},
   "source": [
    "# Query and Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f535d",
   "metadata": {},
   "source": [
    "We don't want wei to be uniform because diffferent tokens will attend to diff tokens in a data dependent manner e.g. a vowel needs to know what are the consonants in its past<br>\n",
    "Self-attention solves this through two vectors - key and query - emitted by an input vector <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a7acd",
   "metadata": {},
   "source": [
    "A query vector has the information we are looking for and a key vector has the information contained by the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2d071",
   "metadata": {},
   "source": [
    "Affinity between the sequences is a dot product between the keys and the queries <br>\n",
    "A query dot products with all the keys of all other tokens and the dot product becomes wei <br>\n",
    "If a key and query align, it interacts at a very high amount and learns more about that specific token as opposed to other tokens in the sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50853567",
   "metadata": {},
   "source": [
    "Implementing a Single Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b6ab976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 \n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single head performing self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # direct MatMul without bias\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# at this stage, no communication has happened between k and q\n",
    "# now, we make them communicate\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d207f",
   "metadata": {},
   "source": [
    "Now, wei has a dynamic shape within batches that are based on token sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83bd0f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282c099",
   "metadata": {},
   "source": [
    "Let's consider the 8th token on the last row <br>\n",
    "It knows what content it has and the position it's in. <br>\n",
    "Based on this, it creates a query \"I'm a vowel, and I'm looking for any consonants at position up to X\" <br>\n",
    "All nodes will emit keys and maybe one of the nodes will emits keys that respond to the query \"I am a consonant and I am in a position up to X\" <br>\n",
    "That key will have a high number in that specific channel, thus creating a high affinity e.g. on the last row, token 0.2297 and 0.2423 are of interest to the 8th token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1605eb",
   "metadata": {},
   "source": [
    "When we do the aggregation, we don't actually aggregate on the tokens x. We aggregate on the 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1310df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 \n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single head performing self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # direct MatMul without bias\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x)\n",
    "\n",
    "# at this stage, no communication has happened between k and q\n",
    "# now, we make them communicate\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33965274",
   "metadata": {},
   "source": [
    "# Notes on Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88fd5f",
   "metadata": {},
   "source": [
    "We can think of x as private to the token e.g. I'm the 5th token with some identity, my information is kept in vector x <br>\n",
    "For the purpose of a single head, here's what I'm interested in (q), here's what I have (k), and if you find me interesting, here's what I will communicate with you (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57874fd2",
   "metadata": {},
   "source": [
    "Attention can be seen as a communication mechanism between a number of nodes in a directed graph. <br>\n",
    "Every node has a vector of information and it gets to aggregate the information via a weighted sum of all the nodes that point to it (in a data dependent manner) <br>\n",
    "Our graph has 8 nodes (block size=8). The first node is pointed to by itself, the second node pointed to the first node and itself and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66dc4b",
   "metadata": {},
   "source": [
    "Attention also has no spatial information, so we must encode the positional information to the nodes <br>\n",
    "This is different from convolution, which has a sort of layout of information in the space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2c015",
   "metadata": {},
   "source": [
    "Elements across batch dimensions never talk to each other and are processed independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f647aa3",
   "metadata": {},
   "source": [
    "In some cases, we want nodes to talk to each other fully i.e. not limited to past nodes. e.g. sentiment analysis <br>\n",
    "This full-ended communication is done by eliminating the masking wei = wei.masked_fill(tril == 0, float('-inf')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483024c",
   "metadata": {},
   "source": [
    "Self attention vs Cross Attention <br>\n",
    "In [our] self attention, the k, q, v all come from the same source (x), so the nodes are self-attending. <br>\n",
    "In cross attention, attention can have q from x and k, v come from some separate source nodes. We produce queries and read information from other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf59992",
   "metadata": {},
   "source": [
    "In the reference paper, we divide the softmax by sqrt(d) - referred to as scaled attention. <br>\n",
    "This normalization prevents extremeties which result in softmax producing skewed values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23a06ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.2000, val loss 4.2047\n",
      "Step 500: train loss 2.6911, val loss 2.7087\n",
      "Step 1000: train loss 2.5196, val loss 2.5303\n",
      "Step 1500: train loss 2.4775, val loss 2.4829\n",
      "Step 2000: train loss 2.4408, val loss 2.4523\n",
      "Step 2500: train loss 2.4272, val loss 2.4435\n",
      "Step 3000: train loss 2.4130, val loss 2.4327\n",
      "Step 3500: train loss 2.3956, val loss 2.4212\n",
      "Step 4000: train loss 2.4041, val loss 2.3992\n",
      "Step 4500: train loss 2.3980, val loss 2.4084\n",
      "\n",
      "UNTGULOK:\n",
      "MI foenderst el\n",
      "O urfrnievil:\n",
      "Alesk, COI yeg agnell thre Mtecoror shad ge?\n",
      "\n",
      "\n",
      "ONGreothakechous omou mpery waly, the oube, er sickes bokecard ihiceny\n",
      "Bing?\n",
      "\n",
      "Al fe of ise fre lbustsel withous; to. Com artl at;\n",
      "I me ffaruk monden itheland'l oer oghithet f, bad gien yof thougre yucouler asureis, nt rt hingesty ckield, wins, in mamy thavenyongmeroe, dojooutthendy sak shil brves\n",
      "GHaraster him to, oupomp rede ds it hor avit gin LUSan thoms lathind my ouerouer aby any sot,\n",
      "K thicerare.\n",
      "\n",
      "I IS:\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # create a buffer which we call tril \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "    \n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to fit into block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1622ccc",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e818ec",
   "metadata": {},
   "source": [
    "Multiple self attention running in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef73fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.2227, val loss 4.2226\n",
      "Step 500: train loss 2.6592, val loss 2.6733\n",
      "Step 1000: train loss 2.4980, val loss 2.5064\n",
      "Step 1500: train loss 2.4291, val loss 2.4349\n",
      "Step 2000: train loss 2.3716, val loss 2.3844\n",
      "Step 2500: train loss 2.3417, val loss 2.3561\n",
      "Step 3000: train loss 2.3149, val loss 2.3347\n",
      "Step 3500: train loss 2.2918, val loss 2.3171\n",
      "Step 4000: train loss 2.2895, val loss 2.2868\n",
      "Step 4500: train loss 2.2748, val loss 2.2858\n",
      "\n",
      "UNTGUST:\n",
      "Ye pich heist el\n",
      "O dof Wie by to yok, CO, tea agethe torr gaecoror cund ge?\n",
      "Wen, reath LoD of youriompery wallav I cou hater sickes\n",
      "Tokt ard dhiceny\n",
      "Bo, troel fef gaise fre le stselant'dcus;\n",
      "I mey\n",
      "Thavely ourind houjeris, anrntit.\n",
      "\n",
      "FAn's of I gimy.\n",
      "\n",
      "Q:\n",
      "That gientyou thoughe yurouler'dsureis lat riky nok thackield, wits, in mamy thout yougmeroe, do of that,\n",
      "Nown\n",
      "Hefil breche warlster him to, oupomp rete dat thim gaikt gin Theandersts lay ind my woed ourse I toy son,\n",
      "KGo,-O'd arir notesed\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # create a buffer which we call tril \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # we have 4 comm channels, we want 8-dim self-attention so it concats to n_embed=32\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to fit into block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca67e5b",
   "metadata": {},
   "source": [
    "# Adding the Feedforward Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a9a1d",
   "metadata": {},
   "source": [
    "In the multi-head, we had the attention that performed communication, but we were too fast in logit calculation. <br>\n",
    "The feed forward single layer performs a feed forward operation on the logits <br>\n",
    "The self attention is the communication, and once that is done for all tokens, the nodes need to think on the data individually - which is what Feed Forward does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a72f0b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.1996, val loss 4.1995\n",
      "Step 500: train loss 2.5993, val loss 2.6077\n",
      "Step 1000: train loss 2.4628, val loss 2.4651\n",
      "Step 1500: train loss 2.3975, val loss 2.3952\n",
      "Step 2000: train loss 2.3298, val loss 2.3472\n",
      "Step 2500: train loss 2.3011, val loss 2.3215\n",
      "Step 3000: train loss 2.2830, val loss 2.2928\n",
      "Step 3500: train loss 2.2494, val loss 2.2716\n",
      "Step 4000: train loss 2.2432, val loss 2.2457\n",
      "Step 4500: train loss 2.2291, val loss 2.2411\n",
      "\n",
      "Fo saw prue to chatist eis yu friie hy tolesk, COICHed agetle torrigkect or cund to of O, rioth Loch,\n",
      "Wel on mpend wallav he ou here. Pickes boktheall-hice: is ow?\n",
      "\n",
      "Af sef awith hee letst tlowt' cull to loce artly ould mouds fris, and fithell wel of soghit.\n",
      "\n",
      "QUS:\n",
      "Thougen.\n",
      "Why, to,\n",
      "Brouck lead,\n",
      "uqiis, notrrayf\n",
      "\n",
      "DUCOLIUCONCENCHINIBSI mamy thave yougmy, eord Vofett;\n",
      "Afrest\n",
      "Hefquind, have allles him to, oupped rote dat thim gack fathrin'd\n",
      "tursts lathise my doed our chalvoy son,\n",
      "Kathit dire.\n",
      "In ther \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # create a buffer which we call tril \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, n_embd), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # we have 4 comm channels, we want 8-dim self-attention so it concats to n_embed=32\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to fit into block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f9078",
   "metadata": {},
   "source": [
    "# Transformer: Communication + Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1cb88",
   "metadata": {},
   "source": [
    "<img src='transformer.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca60cb",
   "metadata": {},
   "source": [
    "We now intersperse communication with computation <br>\n",
    "This is done through a transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8733c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # create a buffer which we call tril \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, n_embd), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    # Transformer block: communication followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to fit into block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3970a9a",
   "metadata": {},
   "source": [
    "Our network becomes a deep neural network and suffers from optimization problems <br>\n",
    "The paper suggests two otimisations to help with the depth of the network: 1) residual connections; 2) Layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15291e20",
   "metadata": {},
   "source": [
    "# Residual Connections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f222a3",
   "metadata": {},
   "source": [
    "Transform the data, and add a skip connection from the previous features. <br>\n",
    "There is a residual pathway that can be forked from to perform additional computation and the resulting computation is added to the original pathway. <br>\n",
    "This is useful because addition distributes gradients equally to both of its branches during backprop <br>\n",
    "The gradients from the loss hops unimpeded from the loss to the input through the residual connection <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3d26676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.6255, val loss 4.6233\n",
      "Step 500: train loss 2.3885, val loss 2.3850\n",
      "Step 1000: train loss 2.2705, val loss 2.2691\n",
      "Step 1500: train loss 2.1873, val loss 2.2097\n",
      "Step 2000: train loss 2.1481, val loss 2.1844\n",
      "Step 2500: train loss 2.1071, val loss 2.1539\n",
      "Step 3000: train loss 2.0710, val loss 2.1441\n",
      "Step 3500: train loss 2.0611, val loss 2.1178\n",
      "Step 4000: train loss 2.0285, val loss 2.1121\n",
      "Step 4500: train loss 2.0038, val loss 2.1022\n",
      "\n",
      "Upastar duke is beed their your tie.\n",
      "\n",
      "DUKE VINENO:\n",
      "Heg agle with shease, make ad gequne am,\n",
      "To you wousel in mades way as he bubserer sidels beked, dliht thy\n",
      "But twoell.\n",
      "\n",
      "VINCENTE:\n",
      "Ole strelaw!' cust to lace and thould moust frised drefither haply enep is with madies itn you the good deatle\n",
      "Theumeing not thy presty cried mayses, and bey that my thmeroed do of that, your\n",
      "Hefor bracimay all wase, and to pomplectes a me his as this readanters,\n",
      "O, this Kertured head by to camp,\n",
      "Kat, the and the be f\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # create a buffer which we call tril \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size, n_embd) # projection into the residual pathway\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd), # the dimensionality is multiplied by 4 as in the paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd) # projection layer\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    # Transformer block: communication followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to fit into block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c90d9",
   "metadata": {},
   "source": [
    "# Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a597c8",
   "metadata": {},
   "source": [
    "This is similar to batch-norm but normalized row- <br>\n",
    "Because the computation does not span across examples, we can delete all the buffers. <br>\n",
    "Similarly, not distinction between training and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58cac313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:  \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps # epsilon\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a7e31",
   "metadata": {},
   "source": [
    "# Side note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19fea0",
   "metadata": {},
   "source": [
    "Very few changes have been made on the transformer since its inception <br>\n",
    "One thing that departs from papers is the position of the Layer Norm - this has recently been implement before the transformation (pre-norm shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "167fd6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.3103, val loss 4.3097\n",
      "Step 500: train loss 2.3999, val loss 2.4008\n",
      "Step 1000: train loss 2.2647, val loss 2.2663\n",
      "Step 1500: train loss 2.1659, val loss 2.1890\n",
      "Step 2000: train loss 2.1309, val loss 2.1676\n",
      "Step 2500: train loss 2.0808, val loss 2.1295\n",
      "Step 3000: train loss 2.0515, val loss 2.1242\n",
      "Step 3500: train loss 2.0438, val loss 2.1036\n",
      "Step 4000: train loss 2.0123, val loss 2.0929\n",
      "Step 4500: train loss 1.9912, val loss 2.0951\n",
      "\n",
      "Forst.\n",
      "\n",
      "MENENCESSTPRIARD OLANUS:\n",
      "Pealy to yoker Offeegeage, Tis me goect you had genone sir\n",
      "They scalian, in make that an he oubsere. Pick shalk, and dhich you back ellought is of bulb strelant's sir to. To goves as and should king, here selp welt ene?\n",
      "Wither for head tnotor,\n",
      "I'll may could\n",
      "Theureis on array nok?\n",
      "Thereel me sead in mamany him your myor, do of that, your\n",
      "Hefquiter make all was im to, and my reat dapies'd gack.\n",
      "\n",
      "WESTUS:\n",
      "Werses laguies my doed head had oy son,\n",
      "Kameiver, eye not sef\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # create a buffer which we call tril \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size, n_embd) # projection into the residual pathway\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd), # the dimensionality is multiplied by 4 as in the paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd) # projection layer\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    # Transformer block: communication followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection and layer norm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to fit into block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859ee9e",
   "metadata": {},
   "source": [
    "# Scaling Up the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c22309",
   "metadata": {},
   "source": [
    "A few cosmetic changes have been added to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # num of independent sequences processed in parallel\n",
    "block_size = 8 # max context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get all unique chars in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder\n",
    "\n",
    "# train test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # create a buffer which we call tril \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size, n_embd) # projection into the residual pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd), # the dimensionality is multiplied by 4 as in the paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd), # projection layer\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    # Transformer block: communication followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection and layer norm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # token reads off the logits for the next token from look-up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # we also embed the position of the tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # then we pass the token embedding and positional embedding\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to fit into block size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus on last time step \n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # get probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # eval loss on train and val sets every once a while\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f390bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But what thou, madedine;\n",
      "And his to pentieto bear moster, and from buke'\n",
      "Puuch;\n",
      "GoZ of our I law yor all har were thou hose, houlds it? arm, wo preade. To Dugas you priightne.\n",
      "\n",
      "LOUCEOSS I'lby apidy winlish our when to tour? mace can trillous,\n",
      "And chears'st hear of a lirunce me to that not ajave, that all all ore? I my repowers!\n",
      "That so of more to goive mine the coldor. Frohtses as that will of am-dear.\n",
      "WAROVINCUS:\n",
      "Hesere hath, breli,\n",
      "Whith dabibitcllied of thigh dister fa: And to bit llan Theit thy poor fe arquresing the the arewof.\n",
      "But me\n",
      "To\n",
      "The foldwer's in hirh crayes?\n",
      "\n",
      "LUTHERN MINTIOLEA:\n",
      "Whis fars. Thes; hild with not this beMace him, as mict the the keet\n",
      "Thou shall of hour you here counse:\n",
      "Pearst larde; whines\n",
      "Your quich madsire tear Sood this and welish, and in that I shall sike my you, thy, sashorticesing I ware hear pan mur?\n",
      "The muse my nead mare!\n",
      "\n",
      "Twor hould mothte; and like hart beartiond!\n",
      "Shald lans.--\n",
      "ClommessIy,\n",
      "Thy, ope, the proothes like: you for it thel do, wed ad man:\n",
      "With marts.\n",
      "Thou lord moser.\n",
      "\n",
      "DUICIONIUS:\n",
      "For I belit tay, brourded\n",
      "thou the riping to brior fitild namure.\n",
      "\n",
      "Shalinilf.\n",
      "\n",
      "POMEOLAULIUS: or Ginker her it in word.\n",
      "\n",
      "Seir, in Cornitles I garkem to bechere more,\n",
      "Un whire wice upbrons\n",
      "Jordsh you? Wifropinf'd?\n",
      "\n",
      "ERY DUS:\n",
      "My his a pinige, in you resciett an have I k he is bard 'to folureive this have a\n",
      "keepoke-\n",
      "A'le well.\n",
      "\n",
      "First and the seeartanhord,\n",
      "You, I dexplenerrace lok, thou\n",
      "kdother.\n",
      "\n",
      "Butdity tofen:\n",
      "Ally thid CAttrong thou meen, I but to bley, en's grow hee, thus.\n",
      "Beat has and the artong this furte agut fhative I knother,\n",
      "Who nother is feart breave yer!\n",
      "\n",
      "Se Theard\n",
      "To be thoushery!\n",
      "Geicircct, bear acin ones I her bugking love naws? harkinich-minsy be tattes:\n",
      "An the breand.\n",
      "\n",
      "BNRIOLLWANTIUS:\n",
      "Parnay! theere and the groan, the heaved hom; thy do, kees,\n",
      "And fice that proatotey\n",
      "Thun; I hear the ward morkisings mey our kine!\n",
      "Murser:\n",
      "We hell timse: not so are saud have good no kursain.\n",
      "\n",
      "Dong It croceapines, hadw,\n",
      "Withtiler in cato hom; wect spershor ot hould batter, a\n",
      "por to swith a fathe so manty in the, whell, kend,\n",
      "End thee me thine.\n",
      "\n",
      "RELAPULEO:\n",
      "Were be thkell I dow it mace,\n",
      "Thy the tilcurtent ewmaned's ow !\n",
      "\n",
      "VIMENxIUS:\n",
      "My, me, my defrade iaf not how the dove to I boy must dut: bid modiser:\n",
      "They, adn fije on by,arrom one theer'd slove.\n",
      " hark manisilate,\n",
      "Marde, in to peake, I moor lave ic all for a mucking the blould's bath belooy woldities the ki's faithts kalnot. aweniess hrom the helt dept Gricyed nold gea thleble\n",
      "droden your thou they me thereast of\n",
      "Eld there pace: of utis\n",
      "Is clove:\n",
      "Thy, I call the useard, I like and are toher? way folse, there that his na:\n",
      "be;\n",
      "Tup ay this shall ho worddOrough\n",
      "Eenssedart'd youshfrajeap on our wath coudinows o futer a efel\n",
      "Fir Is\n",
      "Than a the gieght-undosit wertiss;\n",
      "As batderan:\n",
      "The meets and that nest a in visgain but. Lord I so low, my parting fhathing havond ared is agan to dinest in thee\n",
      "graver, plies the?\n",
      "Cloom puse in stond-forrcy I as the boke,\n",
      "Or came, butil.\n",
      "\n",
      "To hoi the that they, and I go to our,\n",
      "The have for buck, in flaius me wars this fordh kere your and in the poer it\n",
      "Frone horine fror gord exes is no with nath in by of a the if to rike pelaruch'd. same seak us wooth of dasitions the fread\n",
      "And mack,\n",
      "And that mother Julow, naw lavest uponge, that dain\n",
      "shalt Trenoth rake beinad,\n",
      "This such'd:\n",
      "Or wardst: not sieve the hath thine wilf thou he GAND:\n",
      "Ay, my yenect:\n",
      "Awich user,\n",
      "And do!\n",
      "Picemp no nife the croumfe?\n",
      "\n",
      "QUEENTHESBANTAS:\n",
      "Whine it flork\n",
      "ButLer;\n",
      "To, me arto af light thear plar,\n",
      "Your\n",
      "Readip:\n",
      "Cive band,\n",
      "Bot?\n",
      "Wince haws yor arowere\n",
      "GRIK:\n",
      "Purcwite; and dut yo corctelong your loast to the pherts: respone, my the wars dicting?\n",
      "There, and ary lovees thy thar the hake ted as\n",
      "that morrose eale fordorm I thou no, steeccond Go Clorndort, where'l\n",
      "That kobersIiof the her ove aths lordithes cark dadot up.\n",
      "\n",
      "Fill net the cuudtand, I coulcious appinng!\n",
      "Ay waid her ourt bust a well so kano lage ics to of nike yewhters,\n",
      "Her ove youry vitorcomn more I lipting is all't\n",
      "Thou oner him.\n",
      "\n",
      "HOFXANTES\n",
      "Bedes tiert, and the scared so ep the sir, it an sa sight provan:\n",
      "Uncer\n",
      "Wirther:\n",
      "Are hind mam,\n",
      "As noy sor,\n",
      "Thide the purmeed\n",
      "To if hands hould, as said here!\n",
      "\n",
      "ASCAgens, with this will up them,\n",
      "A: crond pragt. Lever'd is not that is, to this of the I ford\n",
      "Tay,\n",
      "And pureen\n",
      "There jeartermWet; i\n",
      "statirngen the more lack a me?\n",
      "\n",
      "LOESOLARAMUS:\n",
      "An mare hort's.\n",
      "\n",
      "ANOLIO:\n",
      "Nird Lordly,\n",
      "That\n",
      "he courdower. Mu peace,\n",
      "As hear's siless with 's gogentl most do more\n",
      "ithe more a plient and saver heart-oull sherureh than shat but\n",
      "sighter makes near thinate tea; it i, and ath? grow'd rascon maiss\n",
      "Corly,\n",
      "Us wize?\n",
      " so faurdon say,\n",
      "With, loke; and I trown!\n",
      "TwOo the speay bonesised!\n",
      "Than fair A cpackes and deabst befters? of I do the compers lection as croce, whe mened\n",
      "Bead! I'an thus; morto you sighge,\n",
      "And ruplos youl her vousmirinsy modere you shall do me usintrard,\n",
      "Sast, ILo chanse: he hars.\n",
      " as will word, save again, thou wa capring me ad sicd and ove of bellar, hou is to beforst to, here is\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0259a",
   "metadata": {},
   "source": [
    "# Encoder vs Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5eb612",
   "metadata": {},
   "source": [
    "<img src='transformer.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcb832",
   "metadata": {},
   "source": [
    "The code implementation is a decoder-only transformer. This focuses on just generating text (autoregressively) <br>\n",
    "We just use the triangular mask from the attention <br>\n",
    "The original paper has an encoder decoder because of its usecase for translation. It expects an encoded French corpus which it is uses to start of the generation using a start and end token. <br>\n",
    "The generation in that case is conditioned on the French corpus. The encoder encodes the French tokens into vector encodings. <br>\n",
    "This encoded information is fed into the cross-attention layer i.e. the queries come from x, but the keys and values come from the encodings <br>\n",
    "This conditions the decoding not just on the past, but also on the fully encoded French text (because full context is necessary for translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
