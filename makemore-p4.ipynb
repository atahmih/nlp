{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering BackProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting into Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function to compare manual gradients to Torch gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cmp(s, dt, t):\n",
    "#     ex = torch.all(dt == t.grad).item()\n",
    "#     app = torch.allclose(dt, t.grad)\n",
    "#     maxdiff = (dt - t.grad).abs().max().item()\n",
    "#     print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-using our network design. The initialisations are changed to small random numbers. Just using random numbers might result in zeros being init, which will be some sort of mask. So multiply the init vars by a small number to prevent this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10 # dimensionality of the char embedding vectors\n",
    "n_hidden = 200 # number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1 = torch.randn((n_embd*block_size, n_hidden), generator=g) * (5/3) / ((n_embd*block_size)**0.5)  # Kaiming init\n",
    "b1 = torch.randn(n_hidden,                      generator=g) * 0.1 # this is generally not needed, since have batchnorm but it will be used for sanity check\n",
    "W2 = torch.randn((n_hidden, vocab_size),        generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                    generator=g) * 0.1\n",
    "\n",
    "# Batch Norm params\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1\n",
    "bnmean_running = torch.zeros((1, n_hidden)) # init mean should be roughly 0\n",
    "bnstd_running = torch.ones((1, n_hidden))  # and std should be roughly 1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bring back an explicit implementation of the loss function. We also break down into smaller chunks with intermediate tensors so we can do backward more appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "# Linear layer\n",
    "hprebn = embcat @ W1 + b1\n",
    "\n",
    "# Batchnorm layer\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1 / (n-1) * (bndiff2).sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non linearity\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# Cross entropy loss \n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean() # iterate down the rows and get the index specified by the element in the tensor Yb, get the mean and negate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass, we want to retain the grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8279, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes,\n",
    "          logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff, bndiff2, hprebn, bnmeani, \n",
    "          embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually backprop through all the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our loss is loss = -1/3a + -1/3b + -1/3c, the derivatives dloss/da = -1/3, dloss/db=-1/3, dloss/dc=-1/3. Now, if our loss has n elements, we see that the derivative essentially becomes dloss/d(n) = -1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. for every element i in loss, dloss/di = -1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But logprobs is an array of shape [32, 27], but only 32 of them participate in loss calculation. The gradient of all these others is 0 because they do not participate in the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs =   torch.zeros_like(logprobs) # holds the derivative of the loss wrt all elements of logprobs\n",
    "\n",
    "# we need to set the derivatives of each element in exactly its same location\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "\n",
    "\n",
    "# we check it using our function cmp\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are doing chain rule backwards, which means dx/dprobs = dlogprobs/dprobs * dx/dlogprobs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dlogprobs/dprobs = d(log(probs)) = 1/probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your probs is very close to 1 (correct character getting high prob), then dprobs becomes 1/1*dlogprobs. Meaning logprobs just gets passed through. But if it is close to 0 (correct character getting low prob), then 1/~0*logprobs boosts itss gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap the loss function and understanding how to work with counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revise the loss. We have the logits coming from the last layer, from which we find the maximum from the row. We then normalize it and exponientiate (for safety)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exponentiate the sum of logits to create count and we normalize so all probs sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to backprob into counts_sum_inv and then into counts. But counts is [32,27] and counts_sum_inv is [32,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this difference in shape, there's automatic broadcasting that looks like the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = a * b, where a[3x3] and b[3,1] <br>\n",
    "It gets broadcast as <br>\n",
    "a11*b1 a12*b1 a13*b1 <br>\n",
    "a21*b2 ..     a23*b2 <br>\n",
    "a31*b3  ..    a33*b3  <br>\n",
    "c[3x3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the derivative, we need to consider the sequential operations done in this broadcast. 1) PyTorch replicates the vector b across all the rows; and 2) the multiplication <br>\n",
    "We first backprob through the multiplication <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at it as scalars c = a * b <br>\n",
    "dc/db = a <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this logic on probs = counts * counts_sum_inv, <br>\n",
    "dloss/dcounts_sum_inv = dloss/dprobs * dprobs/dcounts_sum_inv <br>\n",
    "= dprobs * counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since counts_sum_inv is the repeated vector (broadcast), its gradients are summed up to the current node at which we are backproping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we backprob into the other variable counts i.e dc/da = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counts_sum_inv depends on count in an extra dependency through counts_sum. This creates a second branch of counts, and so we cannot directly verify cmp(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counts_sum_inv = counts_sum^-1 = 1/counts_sum <br>\n",
    "dloss/dcounts_sum = dloss/dcounts_sum_inv * dcounts_sum_inv/dcounts_sum <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But d(1/x) = -1/(x)^2 <br>\n",
    "dcounts_sum = dcounts_sum_inv * -1/(counts_sum)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv \n",
    "\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go to counts_sum = counts.sum(1, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are summing counts.sum along the rows. This means counts [32,27] is summed into counts_sum [32, 1]. Essentially looks like <br>\n",
    "a11 a12 a13 ---> b1 (= a11 +...+ a13) <br>\n",
    "a21 ..  a23 ---> b2 (= a21 +...+ a23) <br>\n",
    "a31 ..  a33 ---> b3 (= a31 +...+ a33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to expand counts_sum col into a 2D array. We have the derivatives wrt b i.e we dcounts_sum. We want this derivative wrt a, i.e how does it behave wrt a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see b_i only depends on a_ij, where j=cols. e.g. b1 only depends on a_1j and not on a_2j or a_3j. db1 wrt to a2j and a3j is 0 (no dependency) <br>\n",
    "Also, if x = y + z, where x and y are scalars, dx/dy = 1 and dx/dz = 1 <br>\n",
    "This derivative of 1 basically serves as a router, taking the incoming derivative and equally re-routing it to participating elements. i.e. b_i routes the incoming derivative to a_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will be taking dcounts_sum and replicating it 27 times horizontally to create [32,27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dcounts = torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But dcounts was previously calculated. so we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for norm_logits, d(e^x) = e^x <br>\n",
    "dloss/dnorm_logits = dloss/dcounts * dcounts/dnorm_logits <br>\n",
    "dcounts/dnorm_logits = norm_logits.exp() = counts ...<br>\n",
    "..because counts = norm_logits.exp() <br>\n",
    "So dcounts/dnorm_logits = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to norm_logits = logits - logit_maxes. Here the shapes are not the same <br>\n",
    "logits [32,27] and logit_maxes [32,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have c[3x3] = a[3x3] - b[3,1] <br>\n",
    "Every c_ij = a_ij - b_i e.g. c_32 = a_32 - b_3 <br>\n",
    "So dc_ij/da_ij = 1 and dc_ij/db_i = -1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we backprop the second branch of logits: logit_maxes = logits.max(1, keepdim=True).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max function returns both the max values and the indices where it found those values. In the forward pass, we only need these max values, but in the backward pass, we also need to know those indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward pass, the max value of each row gets plucked out. In the backward pass, we need to take the dlogit_maxes and scatter it from where the max value originally came"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a one_hot encoding, yielding a 1 where the max came from in each row and we multiply by the dlogit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer 2 <br>\n",
    "logits = h @ W2 + b2 <br>\n",
    "logits[32,27], h[32,64], W2[64,27], b2[27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h * W2 will give a [32,27]. <br>\n",
    "The rules of broadcasting means that b2 gets a padded dim becoming a row vector [1,27] <br>\n",
    "This gets replicated row wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = a @ b + c <br>\n",
    "|d11 d12| = |a11 a12| |b11 b12| + |c1 c2| <br>\n",
    "|d21 d22| = |a21 a22| |b21 b22| + |c1 c2| <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d11 = a11b11 + a12b21 + c1 <br>\n",
    "d12 = a11b12 + a12b22 + c2 <br>\n",
    "d21 = a21b11 + a22b21 + c1 <br>\n",
    "d22 = a21b12 + a22b22 + c2 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want dloss wrt each of the vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dloss/da11 = dloss/d11(b11) + dloss/d11(b12) <br>\n",
    "Continuing for the other vars and summarizing leaves us with <br>\n",
    "dloss/da = dloss/dd @ bT <br>\n",
    "dloss/db = aT @ dloss/dd <br>\n",
    "dloss/dc = dloss/dd * sum(0) i.e. sum across the cols <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shortcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are calculating the derivatives, we need to maintain the shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h is a 32x64 so dh should be 32x64. But we can see that dlogits is 32x27 and W2 is 64x27 <br>\n",
    "Transposing W2 gives us 27x64 and we can then see that <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dh = dlogits @ W2.T <br>\n",
    "Similarly with W2 the shape is 64x27, which can be obtained from h's 32x64 and dlogits' 32x27 as <br>\n",
    "dW2 = h.T @ dlogits <br>\n",
    "And for b2, it's straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('db2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = tanh(z) <br>\n",
    "da/dz = 1 - a^2 <br>\n",
    "Note that a is the output of tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dhpreact = (1.0 - h**2) * dh\n",
    "\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hpreact = bngain * bnraw + bnbias <br>\n",
    "hpreact [32,64] <br>\n",
    "bngain [1,64] <br>\n",
    "bnraw [32,64] <br>\n",
    "bnbias [1,64] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbngain = bnraw * dhpreact <br> \n",
    "bngain needs to be of size [1,64], but bnraw * dhpreact is [32,64] <br>\n",
    "So the gradients in all of those rows need to be summed up (across col) and we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbngain         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnraw          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnbias         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the basic elementwise multiplications, we can repeat the derivatives as we have seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbndiff = bnvar_inv * dbnraw <br>\n",
    "dbnvar_inv = bndiff * dbnraw\n",
    "But bndiff is a [32,64] and dbnraw is [32,64] and dbnvar_inv needs to match dbnvar_inv's [1,64], so we sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbndiff         | exact: False | approximate: False | maxdiff: 0.0012138169258832932\n",
      "dbnvar_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbndiff is not correct because we still have an extra branch of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bnvar_inv = (bnvar + 1e-5)**-0.5 <br>\n",
    "d(x^n)/dx = nx^n-1 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbnvar          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "\n",
    "cmp('dbnvar', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note about broadcast and sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a convention here, everytime there's a broadcast in the forward pass, it becomes a sum in the backward pass and vice versa. The broadcast indicates a variable re-use as the variable is copied multiple times. This has to be summed back to the original variable in the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bnvar = 1 / (n-1) * (bndiff2).sum(0, keepdim=True) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
